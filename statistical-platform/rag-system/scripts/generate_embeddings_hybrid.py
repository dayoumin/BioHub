"""
Generate embeddings for chunks and save to ChromaDB with BM25 index (Hybrid Search)

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:
1. chunks.jsonì—ì„œ 780ê°œ ì²­í¬ ë¡œë“œ
2. Ollama nomic-embed-textë¡œ ì„ë² ë”© ìƒì„± â†’ ChromaDB (Vector Search)
3. BM25 ì¸ë±ìŠ¤ ìƒì„± â†’ JSON íŒŒì¼ (Keyword Search)

Hybrid Search = Vector Search + BM25 Keyword Search + Reranking

Requirements:
- ollama (localhost:11434)
- chromadb
- rank-bm25
- requests
"""

import json
import os
import sys
from pathlib import Path
import time
from typing import List, Dict, Any
import requests
import pickle

# ChromaDB import
try:
    import chromadb
    from chromadb.config import Settings
except ImportError:
    print("âŒ chromadb not installed. Run: pip install chromadb")
    sys.exit(1)

# BM25 import
try:
    from rank_bm25 import BM25Okapi
except ImportError:
    print("âŒ rank-bm25 not installed. Run: pip install rank-bm25")
    sys.exit(1)

# Paths
PROJECT_ROOT = Path(__file__).parent.parent
CHUNKS_FILE = PROJECT_ROOT / "data" / "chunks" / "chunks.json"
VECTOR_DB_PATH = PROJECT_ROOT / "data" / "vector_db"
BM25_INDEX_PATH = PROJECT_ROOT / "data" / "bm25_index.pkl"

# Ollama settings
OLLAMA_ENDPOINT = "http://localhost:11434"
EMBEDDING_MODEL = "nomic-embed-text"

print("ğŸš€ Hybrid Search ì¸ë±ìŠ¤ ìƒì„± ì‹œì‘")
print("=" * 60)
print("ğŸ“‹ êµ¬ì„±:")
print("  1. Vector Search: ChromaDB (Ollama nomic-embed-text)")
print("  2. Keyword Search: BM25 (rank-bm25)")
print("  3. Reranking: Alpha ê°€ì¤‘ì¹˜ ê²°í•©")
print("=" * 60)
print(f"Ollama ì—”ë“œí¬ì¸íŠ¸: {OLLAMA_ENDPOINT}")
print(f"ì„ë² ë”© ëª¨ë¸: {EMBEDDING_MODEL}")
print(f"ì²­í¬ íŒŒì¼: {CHUNKS_FILE}")
print(f"Vector DB ê²½ë¡œ: {VECTOR_DB_PATH}")
print(f"BM25 ì¸ë±ìŠ¤ ê²½ë¡œ: {BM25_INDEX_PATH}")
print("=" * 60)

# Check Ollama server
print("\nğŸ” Ollama ì„œë²„ í™•ì¸ ì¤‘...")
try:
    response = requests.get(f"{OLLAMA_ENDPOINT}/api/tags")
    if response.status_code == 200:
        models = response.json().get("models", [])
        model_names = [m.get("name", "") for m in models]
        if EMBEDDING_MODEL in model_names:
            print(f"âœ… Ollama ì„œë²„ ì—°ê²° ì„±ê³µ ({EMBEDDING_MODEL} ì‚¬ìš© ê°€ëŠ¥)")
        else:
            print(f"âš ï¸ {EMBEDDING_MODEL} ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            print(f"ë‹¤ìš´ë¡œë“œ: ollama pull {EMBEDDING_MODEL}")
            sys.exit(1)
    else:
        print(f"âŒ Ollama ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
        sys.exit(1)
except requests.exceptions.ConnectionError:
    print(f"âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print(f"Ollamaë¥¼ ì‹¤í–‰í•˜ì„¸ìš”: ollama serve")
    sys.exit(1)

# Load chunks
print(f"\nğŸ“š ì²­í¬ ë¡œë”© ì¤‘...")
if not CHUNKS_FILE.exists():
    print(f"âŒ ì²­í¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {CHUNKS_FILE}")
    print("ë¨¼ì € semantic_chunker.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    sys.exit(1)

with open(CHUNKS_FILE, 'r', encoding='utf-8') as f:
    chunks_data = json.load(f)

chunks: List[Dict[str, Any]] = chunks_data.get("chunks", [])
print(f"âœ… ì´ {len(chunks)}ê°œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")

# ============================================================
# Part 1: BM25 Index ìƒì„± (Keyword Search)
# ============================================================
print(f"\nğŸ”¤ Part 1: BM25 ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
print(f"  BM25ëŠ” í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (TF-IDF ê°œì„  ë²„ì „)")

# Tokenize documents for BM25
tokenized_corpus = []
for chunk in chunks:
    content = chunk.get("content", "")
    # Simple tokenization (lowercase + split by whitespace)
    tokens = content.lower().split()
    tokenized_corpus.append(tokens)

# Create BM25 index
bm25 = BM25Okapi(tokenized_corpus)

# Save BM25 index
with open(BM25_INDEX_PATH, 'wb') as f:
    pickle.dump({
        "bm25": bm25,
        "chunks": chunks  # Save chunks for retrieval
    }, f)

print(f"âœ… BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
print(f"  - ì €ì¥ ìœ„ì¹˜: {BM25_INDEX_PATH}")
print(f"  - ì¸ë±ìŠ¤ í¬ê¸°: {BM25_INDEX_PATH.stat().st_size / 1024 / 1024:.2f} MB")

# ============================================================
# Part 2: Vector DB ìƒì„± (Semantic Search)
# ============================================================
print(f"\nğŸ”¢ Part 2: Vector DB ìƒì„± ì¤‘...")
print(f"  ChromaDBëŠ” ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ (ì„ë² ë”© ìœ ì‚¬ë„)")

# Initialize ChromaDB
VECTOR_DB_PATH.mkdir(parents=True, exist_ok=True)

client = chromadb.PersistentClient(
    path=str(VECTOR_DB_PATH),
    settings=Settings(
        anonymized_telemetry=False,
        allow_reset=True
    )
)

# Delete existing collection if any
try:
    client.delete_collection("statistical_docs")
    print("  âš ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œë¨")
except Exception:
    pass

# Create collection
collection = client.create_collection(
    name="statistical_docs",
    metadata={"description": "Statistical documentation from SciPy/NumPy/statsmodels/pingouin"}
)
print(f"âœ… ChromaDB ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")

# Generate embeddings and add to ChromaDB
print(f"\n  ì„ë² ë”© ìƒì„± ì¤‘... (ì´ {len(chunks)}ê°œ ì²­í¬)")

batch_size = 10  # Process in batches
total_batches = (len(chunks) + batch_size - 1) // batch_size

start_time = time.time()
processed = 0
failed = 0

for batch_idx in range(total_batches):
    batch_start = batch_idx * batch_size
    batch_end = min((batch_idx + 1) * batch_size, len(chunks))
    batch_chunks = chunks[batch_start:batch_end]

    # Prepare batch data
    batch_documents = []
    batch_metadatas = []
    batch_ids = []

    for idx, chunk in enumerate(batch_chunks):
        chunk_id = f"chunk_{batch_start + idx}"
        content = chunk.get("content", "")
        metadata = chunk.get("metadata", {})

        # Generate embedding using Ollama
        try:
            embed_response = requests.post(
                f"{OLLAMA_ENDPOINT}/api/embeddings",
                json={
                    "model": EMBEDDING_MODEL,
                    "prompt": content
                },
                timeout=30
            )

            if embed_response.status_code != 200:
                print(f"  ERROR: Embedding failed (Chunk {chunk_id}): HTTP {embed_response.status_code}")
                failed += 1
                continue

            embedding_data = embed_response.json()
            embedding = embedding_data.get("embedding", [])

            if not embedding:
                print(f"  ERROR: Empty embedding (Chunk {chunk_id})")
                failed += 1
                continue

            # Add to batch
            batch_documents.append(content)
            batch_metadatas.append(metadata)
            batch_ids.append(chunk_id)

            processed += 1

        except Exception as e:
            print(f"  âŒ ì„ë² ë”© ìƒì„± ì˜¤ë¥˜ (Chunk {chunk_id}): {str(e)}")
            failed += 1

    # Add batch to ChromaDB
    if batch_documents:
        try:
            collection.add(
                documents=batch_documents,
                metadatas=batch_metadatas,
                ids=batch_ids
            )
        except Exception as e:
            print(f"  âŒ ChromaDB ì €ì¥ ì˜¤ë¥˜ (Batch {batch_idx+1}): {str(e)}")
            failed += len(batch_documents)
            processed -= len(batch_documents)

    # Progress
    if (batch_idx + 1) % 10 == 0:
        elapsed = time.time() - start_time
        rate = processed / elapsed if elapsed > 0 else 0
        eta = (len(chunks) - processed) / rate if rate > 0 else 0
        print(f"    ì§„í–‰: {processed}/{len(chunks)} ({processed/len(chunks)*100:.1f}%) | {rate:.1f} chunks/sec | ETA: {eta/60:.1f}ë¶„")

# Final summary
end_time = time.time()
elapsed = end_time - start_time

print(f"\nâœ… Vector DB ìƒì„± ì™„ë£Œ")
print(f"  - ì„±ê³µ: {processed}/{len(chunks)} ({processed/len(chunks)*100:.1f}%)")
print(f"  - ì‹¤íŒ¨: {failed}")
print(f"  - ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ ({elapsed/60:.1f}ë¶„)")
print(f"  - ì²˜ë¦¬ ì†ë„: {processed/elapsed:.1f} chunks/sec")

# Verify ChromaDB
print(f"\nğŸ” ChromaDB ê²€ì¦ ì¤‘...")
count = collection.count()
print(f"  âœ… ì €ì¥ëœ ë²¡í„° ìˆ˜: {count}")

if count != processed:
    print(f"  âš ï¸ ê²½ê³ : ì €ì¥ëœ ë²¡í„° ìˆ˜({count})ì™€ ì²˜ë¦¬ëœ ì²­í¬ ìˆ˜({processed})ê°€ ë‹¤ë¦…ë‹ˆë‹¤!")
else:
    print(f"  âœ… ëª¨ë“  ì„ë² ë”©ì´ ì •ìƒì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

# ============================================================
# Final Summary
# ============================================================
print("\n" + "=" * 60)
print("âœ… Hybrid Search ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")
print("=" * 60)
print("ğŸ“Š ìƒì„±ëœ ì¸ë±ìŠ¤:")
print(f"  1. Vector DB (ChromaDB): {count}ê°œ ë²¡í„°")
print(f"     - ê²½ë¡œ: {VECTOR_DB_PATH}")
print(f"     - ìš©ë„: ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ (ì„ë² ë”© ìœ ì‚¬ë„)")
print(f"  2. BM25 Index: {len(tokenized_corpus)}ê°œ ë¬¸ì„œ")
print(f"     - ê²½ë¡œ: {BM25_INDEX_PATH}")
print(f"     - ìš©ë„: í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (TF-IDF)")
print("=" * 60)
print("ğŸ“ ì‚¬ìš© ë°©ë²•:")
print("  1. ì‚¬ìš©ì ì¿¼ë¦¬ ì…ë ¥")
print("  2. Vector Search â†’ Top K ê²°ê³¼ (ì˜ë¯¸ ìœ ì‚¬ë„)")
print("  3. BM25 Search â†’ Top K ê²°ê³¼ (í‚¤ì›Œë“œ ë§¤ì¹­)")
print("  4. Reranking â†’ Alpha ê°€ì¤‘ì¹˜ ê²°í•©")
print("     - Final Score = alpha * vector_score + (1-alpha) * bm25_score")
print("     - ê¶Œì¥: alpha = 0.7 (Vector 70%, BM25 30%)")
print("=" * 60)
print("\nâœ… Week 2 Day 3-4 ì™„ë£Œ!")
print("  ë‹¤ìŒ ë‹¨ê³„: Hybrid RAG ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ (Week 2 Day 5)")
