"""
Vector Store DB ë¹Œë” ìŠ¤í¬ë¦½íŠ¸ (ë©€í‹° ëª¨ë¸ ì§€ì›)

í¬ë¡¤ë§ëœ 111ê°œ ë¬¸ì„œë¥¼ Vector Store DBë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
- SQLite íŒŒì¼ ì•ˆì— ì›ë³¸ ë¬¸ì„œ + ì„ë² ë”© ë²¡í„° ì €ì¥ (í•˜ì´ë¸Œë¦¬ë“œ DB)
- FTS5 ì „ë¬¸ ê²€ìƒ‰ + Vector ì˜ë¯¸ ê²€ìƒ‰ ì§€ì›

ì‹¤í–‰:
    cd statistical-platform/rag-system
    python scripts/build_sqlite_db.py --model mxbai-embed-large
    python scripts/build_sqlite_db.py --model nomic-embed-text
    python scripts/build_sqlite_db.py --model qwen3-embedding:0.6b

ì¶œë ¥:
    data/vector-{model}.db (Vector Store ë°ì´í„°ë² ì´ìŠ¤)
    ì˜ˆ: vector-mxbai-embed-large.db
"""

import os
import sys
import sqlite3
import json
import time
import argparse
from pathlib import Path
from typing import List, Dict, Optional
import hashlib
import struct
import requests

# Windows ì½˜ì†” UTF-8 ì¶œë ¥ ê°•ì œ
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# ì„¤ì •
SCRIPT_DIR = Path(__file__).parent
RAG_SYSTEM_DIR = SCRIPT_DIR.parent
DATA_DIR = RAG_SYSTEM_DIR / "data"
SCHEMA_PATH = RAG_SYSTEM_DIR / "schema.sql"

# Ollama ì„¤ì •
OLLAMA_ENDPOINT = "http://localhost:11434"

# ê¸€ë¡œë²Œ ë³€ìˆ˜ (argparseë¡œ ì„¤ì •ë¨)
EMBEDDING_MODEL = "mxbai-embed-large"  # ê¸°ë³¸ê°’
DB_PATH = DATA_DIR / "rag.db"  # ê¸°ë³¸ê°’

# ë¬¸ì„œ ë””ë ‰í† ë¦¬
DOC_DIRS = {
    "scipy": DATA_DIR / "scipy",
    "numpy": DATA_DIR / "numpy",
    "statsmodels": DATA_DIR / "statsmodels",
    "pingouin": DATA_DIR / "pingouin",
    "project": DATA_DIR / "project",
    "methodology": DATA_DIR / "methodology-guide",
    "openintro": DATA_DIR / "openintro"
}


def create_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ë° ìŠ¤í‚¤ë§ˆ ì ìš©"""
    print(f"[1/4] ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±: {DB_PATH}")

    # ê¸°ì¡´ DB ì‚­ì œ (ì¬êµ¬ì¶•)
    if DB_PATH.exists():
        DB_PATH.unlink()
        print("  - ê¸°ì¡´ DB ì‚­ì œë¨")

    # ìŠ¤í‚¤ë§ˆ ì ìš©
    with open(SCHEMA_PATH, 'r', encoding='utf-8') as f:
        schema_sql = f.read()

    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.executescript(schema_sql)
    conn.commit()
    conn.close()

    print("  - ìŠ¤í‚¤ë§ˆ ì ìš© ì™„ë£Œ")


def generate_doc_id(library: str, filename: str) -> str:
    """ë¬¸ì„œ ID ìƒì„± (ì˜ˆ: scipy_ttest_ind)"""
    # íŒŒì¼ëª…ì—ì„œ .md ì œê±°
    name = filename.replace('.md', '')
    # íŠ¹ìˆ˜ ë¬¸ì ì œê±° ë° ì†Œë¬¸ì ë³€í™˜
    name = name.replace('-', '_').replace('.', '_').lower()
    return f"{library}_{name}"


def extract_title_from_content(content: str, filename: str) -> str:
    """Markdownì—ì„œ ì œëª© ì¶”ì¶œ"""
    lines = content.split('\n')

    # ì²« ë²ˆì§¸ # í—¤ë” ì°¾ê¸°
    for line in lines:
        if line.startswith('# '):
            return line.replace('# ', '').strip()

    # ì œëª©ì´ ì—†ìœ¼ë©´ íŒŒì¼ëª… ì‚¬ìš©
    return filename.replace('.md', '').replace('_', ' ').replace('-', ' ').title()


def extract_summary(content: str, max_length: int = 200) -> str:
    """ìš”ì•½ ìƒì„± (ì²« 200ì)"""
    # Markdown í—¤ë” ì œê±°
    lines = [line for line in content.split('\n') if not line.startswith('#')]
    text = ' '.join(lines).strip()

    # ì²« 200ì ì¶”ì¶œ
    if len(text) > max_length:
        return text[:max_length] + '...'
    return text


def categorize_document(library: str, doc_id: str, content: str) -> Optional[str]:
    """ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
    # í”„ë¡œì íŠ¸ ë¬¸ì„œëŠ” íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ
    if library == "project":
        if "worker1" in doc_id:
            return "descriptive"
        elif "worker2" in doc_id:
            return "hypothesis"
        elif "worker3" in doc_id:
            return "nonparametric_anova"
        elif "worker4" in doc_id:
            return "regression_advanced"
        elif "statistical_methods" in doc_id:
            return "overview"

    # ë°©ë²•ë¡  ê°€ì´ë“œ
    if library == "methodology":
        if "decision" in doc_id or "tree" in doc_id:
            return "guide_method_selection"
        elif "assumption" in doc_id:
            return "guide_assumptions"
        elif "interpretation" in doc_id:
            return "guide_interpretation"
        elif "comparison" in doc_id:
            return "guide_comparison"

    # SciPy/NumPy: í•¨ìˆ˜ëª…ìœ¼ë¡œ ì¶”ì •
    content_lower = content.lower()
    if any(word in content_lower for word in ["test", "ê²€ì •", "hypothesis"]):
        return "hypothesis"
    elif any(word in content_lower for word in ["regression", "íšŒê·€"]):
        return "regression"
    elif any(word in content_lower for word in ["correlation", "ìƒê´€"]):
        return "correlation"
    elif any(word in content_lower for word in ["anova", "ë¶„ì‚°"]):
        return "anova"
    elif any(word in content_lower for word in ["mean", "median", "std", "var"]):
        return "descriptive"

    return None


def count_words(text: str) -> int:
    """ë‹¨ì–´ ìˆ˜ ê³„ì‚°"""
    # ê³µë°± ê¸°ì¤€ ë¶„ë¦¬ (ê°„ë‹¨í•œ ë°©ë²•)
    return len(text.split())


def generate_embedding(text: str) -> Optional[List[float]]:
    """Ollama APIë¥¼ í†µí•´ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ì„ë² ë”© ëª¨ë¸ì€ ë³´í†µ 512 í† í° ì œí•œ)
    MAX_CHARS = 2000
    truncated_text = text[:MAX_CHARS] if len(text) > MAX_CHARS else text

    try:
        response = requests.post(
            f"{OLLAMA_ENDPOINT}/api/embeddings",
            json={
                "model": EMBEDDING_MODEL,
                "prompt": truncated_text
            },
            timeout=30
        )

        if not response.ok:
            print(f"  âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ ({response.status_code}): {response.text[:100]}")
            return None

        data = response.json()
        return data.get("embedding")

    except Exception as e:
        print(f"  âš ï¸ ì„ë² ë”© ìƒì„± ì—ëŸ¬: {e}")
        return None


def embedding_to_blob(embedding: List[float]) -> bytes:
    """ì„ë² ë”© ë²¡í„°ë¥¼ SQLite BLOBìœ¼ë¡œ ë³€í™˜ (float32 ë°°ì—´)"""
    # float32ë¡œ ë³€í™˜ (4ë°”ì´íŠ¸ * 1024 = 4096 ë°”ì´íŠ¸)
    return struct.pack(f'{len(embedding)}f', *embedding)


def blob_to_embedding(blob: bytes) -> List[float]:
    """SQLite BLOBì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³µì›"""
    # BLOB í¬ê¸°ë¡œ ì°¨ì› ê³„ì‚°
    num_dimensions = len(blob) // 4  # 4ë°”ì´íŠ¸ = float32
    return list(struct.unpack(f'{num_dimensions}f', blob))


def load_documents() -> List[Dict]:
    """ëª¨ë“  ë¬¸ì„œ ë¡œë“œ ë° ì„ë² ë”© ìƒì„±"""
    print(f"[2/5] ë¬¸ì„œ ë¡œë“œ ë° ì„ë² ë”© ìƒì„± ì¤‘...")

    documents = []
    current_time = int(time.time())
    embedding_success = 0
    embedding_failed = 0

    for library, doc_dir in DOC_DIRS.items():
        if not doc_dir.exists():
            print(f"  - {library}: ë””ë ‰í† ë¦¬ ì—†ìŒ (ìŠ¤í‚µ)")
            continue

        md_files = list(doc_dir.glob("*.md"))
        print(f"  - {library}: {len(md_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì¤‘...")

        for idx, md_file in enumerate(md_files, 1):
            try:
                # íŒŒì¼ ì½ê¸°
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # ë¬¸ì„œ ID ìƒì„±
                doc_id = generate_doc_id(library, md_file.name)

                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                title = extract_title_from_content(content, md_file.name)
                summary = extract_summary(content)
                category = categorize_document(library, doc_id, content)
                word_count = count_words(content)

                # ğŸ”¥ ì„ë² ë”© ìƒì„± (Ollama API í˜¸ì¶œ)
                print(f"    [{idx}/{len(md_files)}] {doc_id[:30]:30} ... ì„ë² ë”© ìƒì„± ì¤‘", end=" ")
                embedding = generate_embedding(content)

                if embedding:
                    embedding_blob = embedding_to_blob(embedding)
                    embedding_success += 1
                    print(f"âœ“ ({len(embedding)}ì°¨ì›)")
                else:
                    embedding_blob = None
                    embedding_failed += 1
                    print("âœ—")

                # ë¬¸ì„œ ê°ì²´ ìƒì„±
                doc = {
                    "doc_id": doc_id,
                    "title": title,
                    "library": library,
                    "category": category,
                    "content": content,
                    "summary": summary,
                    "source_url": None,
                    "source_file": str(md_file.relative_to(RAG_SYSTEM_DIR)),
                    "created_at": current_time,
                    "updated_at": current_time,
                    "word_count": word_count,
                    "embedding": embedding_blob,
                    "embedding_model": EMBEDDING_MODEL if embedding_blob else None
                }

                documents.append(doc)

            except Exception as e:
                print(f"  âš ï¸ ì—ëŸ¬ ({md_file.name}): {e}")

    print(f"\n  âœ“ ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
    print(f"  âœ“ ì„ë² ë”© ì„±ê³µ: {embedding_success}ê°œ | ì‹¤íŒ¨: {embedding_failed}ê°œ")
    return documents


def insert_documents(documents: List[Dict]):
    """ë¬¸ì„œë¥¼ DBì— ì‚½ì… (ì„ë² ë”© í¬í•¨)"""
    print(f"[3/5] ë¬¸ì„œ DB ì‚½ì… ì¤‘...")

    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    for doc in documents:
        cursor.execute("""
            INSERT INTO documents (
                doc_id, title, library, category,
                content, summary,
                source_url, source_file,
                created_at, updated_at, word_count,
                embedding, embedding_model
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            doc["doc_id"],
            doc["title"],
            doc["library"],
            doc["category"],
            doc["content"],
            doc["summary"],
            doc["source_url"],
            doc["source_file"],
            doc["created_at"],
            doc["updated_at"],
            doc["word_count"],
            doc["embedding"],
            doc["embedding_model"]
        ))

    conn.commit()
    conn.close()

    print(f"  âœ“ {len(documents)}ê°œ ë¬¸ì„œ ì‚½ì… ì™„ë£Œ")


def generate_statistics():
    """DB í†µê³„ ìƒì„±"""
    print(f"[4/5] DB í†µê³„ ìƒì„± ì¤‘...")

    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    # ë¼ì´ë¸ŒëŸ¬ë¦¬ë³„ í†µê³„
    cursor.execute("""
        SELECT library, COUNT(*) as count, SUM(word_count) as total_words
        FROM documents
        GROUP BY library
    """)

    print("\nğŸ“Š ë¬¸ì„œ í†µê³„:")
    print("-" * 50)
    for row in cursor.fetchall():
        library, count, total_words = row
        print(f"  {library:15} | {count:3}ê°œ | {total_words:,}ì")

    # ì „ì²´ í†µê³„
    cursor.execute("SELECT COUNT(*), SUM(word_count) FROM documents")
    total_docs, total_words = cursor.fetchone()
    print("-" * 50)
    print(f"  {'TOTAL':15} | {total_docs:3}ê°œ | {total_words:,}ì")

    # FTS í…Œì´ë¸” í™•ì¸
    cursor.execute("SELECT COUNT(*) FROM documents_fts")
    fts_count = cursor.fetchone()[0]
    print(f"\nâœ“ FTS ì¸ë±ìŠ¤: {fts_count}ê°œ ë¬¸ì„œ")

    # ì„ë² ë”© í†µê³„
    cursor.execute("SELECT COUNT(*) FROM documents WHERE embedding IS NOT NULL")
    embedding_count = cursor.fetchone()[0]
    print(f"âœ“ ì„ë² ë”©: {embedding_count}ê°œ ë¬¸ì„œ ({EMBEDDING_MODEL})")

    # DB íŒŒì¼ í¬ê¸°
    db_size = DB_PATH.stat().st_size / (1024 * 1024)  # MB
    print(f"âœ“ DB í¬ê¸°: {db_size:.2f} MB")

    conn.close()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    global EMBEDDING_MODEL, DB_PATH

    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(
        description='RAG System - SQLite DB Builder (ë©€í‹° ëª¨ë¸ ì§€ì›)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ì˜ˆì œ:
  python scripts/build_sqlite_db.py --model mxbai-embed-large
  python scripts/build_sqlite_db.py --model nomic-embed-text
  python scripts/build_sqlite_db.py --model qwen3-embedding:0.6b
        '''
    )
    parser.add_argument(
        '--model',
        type=str,
        default='mxbai-embed-large',
        help='Ollama ì„ë² ë”© ëª¨ë¸ (ê¸°ë³¸: mxbai-embed-large)'
    )

    args = parser.parse_args()

    # ê¸€ë¡œë²Œ ë³€ìˆ˜ ì„¤ì •
    EMBEDDING_MODEL = args.model

    # ëª¨ë¸ëª…ì—ì„œ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜ (ì½œë¡  ì œê±°)
    model_filename = args.model.replace(':', '-')
    # Vector Store DB íŒŒì¼ëª… (vector- ì ‘ë‘ì‚¬ë¡œ ëª…í™•í™”)
    DB_PATH = DATA_DIR / f"vector-{model_filename}.db"

    print("=" * 60)
    print("RAG System - SQLite DB Builder (Multi-Model Support)")
    print(f"Embedding Model: {EMBEDDING_MODEL}")
    print("=" * 60)
    print()

    # Ollama ì„œë²„ í™•ì¸
    print("[0/5] Ollama ì„œë²„ í™•ì¸ ì¤‘...")
    try:
        response = requests.get(f"{OLLAMA_ENDPOINT}/api/tags", timeout=5)
        if response.ok:
            print(f"  âœ“ Ollama ì„œë²„ ì—°ê²° ì„±ê³µ ({OLLAMA_ENDPOINT})")

            # ëª¨ë¸ ì„¤ì¹˜ í™•ì¸
            models_data = response.json()
            available_models = [m['name'] for m in models_data.get('models', [])]

            # ëª¨ë¸ëª… ë§¤ì¹­ (ë²„ì „ íƒœê·¸ ê³ ë ¤)
            model_exists = any(
                EMBEDDING_MODEL in model_name or model_name.startswith(EMBEDDING_MODEL)
                for model_name in available_models
            )

            if model_exists:
                print(f"  âœ“ ì„ë² ë”© ëª¨ë¸ í™•ì¸: {EMBEDDING_MODEL}")
            else:
                print(f"  âš ï¸ ëª¨ë¸ '{EMBEDDING_MODEL}'ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print(f"  â†’ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: ollama pull {EMBEDDING_MODEL}")
                print(f"  â†’ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {', '.join(available_models)}")
                exit(1)
        else:
            print(f"  âš ï¸ Ollama ì„œë²„ ì‘ë‹µ ì´ìƒ: {response.status_code}")
    except Exception as e:
        print(f"  âŒ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        print(f"  â†’ ì„ë² ë”© ìƒì„±ì´ ìŠ¤í‚µë©ë‹ˆë‹¤.")
    print()

    try:
        # 1. DB ìƒì„±
        create_database()

        # 2. ë¬¸ì„œ ë¡œë“œ ë° ì„ë² ë”© ìƒì„±
        documents = load_documents()

        # 3. DB ì‚½ì…
        insert_documents(documents)

        # 4. í†µê³„ ìƒì„±
        generate_statistics()

        print()
        print("=" * 60)
        print("âœ… DB ë¹Œë“œ ì™„ë£Œ!")
        print(f"   ìœ„ì¹˜: {DB_PATH}")
        print(f"   ì„ë² ë”© ëª¨ë¸: {EMBEDDING_MODEL}")
        print("=" * 60)

    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        exit(1)


if __name__ == "__main__":
    main()
