/**
 * LangGraph vs Langchain ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
 *
 * ëª©ì : ë‘ Providerì˜ ì‹¤ì œ ì„±ëŠ¥ ì°¨ì´ ì¸¡ì •
 *
 * í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:
 * 1. ë™ì¼í•œ ì¿¼ë¦¬ë¡œ ê° Provider í…ŒìŠ¤íŠ¸ (10íšŒ ë°˜ë³µ)
 * 2. ì‘ë‹µ ì‹œê°„, ì„ë² ë”© í˜¸ì¶œ íšŸìˆ˜, ê²€ìƒ‰ í’ˆì§ˆ ë¹„êµ
 * 3. í†µê³„ì  ë¶„ì„ (í‰ê· , í‘œì¤€í¸ì°¨, ìµœì†Œ/ìµœëŒ€)
 *
 * í•„ìš” ì¡°ê±´:
 * - Ollama ì„¤ì¹˜ ë° ì‹¤í–‰ (http://localhost:11434)
 * - ëª¨ë¸ ì„¤ì¹˜: mxbai-embed-large, qwen2.5
 * - Vector DB ì¡´ì¬: /rag-data/vector-qwen3-embedding-0.6b.db
 */

import { RAGService } from '../lib/rag/rag-service.js'

// í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì„¸íŠ¸ (í†µê³„ ê´€ë ¨)
const TEST_QUERIES = [
  'ANOVA ê°€ì • ê²€ì •ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?',
  'ì •ê·œì„± ê²€ì • ë°©ë²•ì—ëŠ” ì–´ë–¤ ê²ƒì´ ìˆë‚˜ìš”?',
  'íšŒê·€ë¶„ì„ì—ì„œ ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í•´ê²°í•˜ë‚˜ìš”?',
  'ì¹´ì´ì œê³± ê²€ì •ì˜ ì‚¬ìš© ì¡°ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?',
  'Mann-Whitney U ê²€ì •ì€ ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”?',
]

/**
 * ì„±ëŠ¥ ì¸¡ì • (ë‹¨ì¼ ì¿¼ë¦¬)
 */
async function measurePerformance(provider, query, providerType) {
  const startTime = Date.now()

  try {
    const response = await provider.query({
      query,
      searchMode: 'hybrid',
    })

    const elapsed = Date.now() - startTime

    return {
      success: true,
      elapsed,
      responseTime: response.metadata?.responseTime || elapsed,
      sourcesCount: response.sources.length,
      answerLength: response.answer.length,
      citedDocsCount: response.citedDocIds?.length || 0,
      providerType,
    }
  } catch (error) {
    return {
      success: false,
      elapsed: Date.now() - startTime,
      error: error.message,
      providerType,
    }
  }
}

/**
 * í†µê³„ ê³„ì‚°
 */
function calculateStats(measurements) {
  const times = measurements.map((m) => m.elapsed)
  const avg = times.reduce((a, b) => a + b, 0) / times.length
  const variance = times.reduce((sum, t) => sum + Math.pow(t - avg, 2), 0) / times.length
  const stdDev = Math.sqrt(variance)

  return {
    avg: Math.round(avg),
    min: Math.min(...times),
    max: Math.max(...times),
    stdDev: Math.round(stdDev),
    count: times.length,
  }
}

/**
 * ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
 */
async function runBenchmark() {
  console.log('ğŸš€ LangGraph vs Langchain ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...\n')

  // Ollama ì—°ê²° í™•ì¸
  try {
    const response = await fetch('http://localhost:11434/api/tags')
    if (!response.ok) {
      throw new Error('Ollama ì„œë²„ ì‘ë‹µ ì—†ìŒ')
    }
    console.log('âœ… Ollama ì„œë²„ ì—°ê²° ì„±ê³µ\n')
  } catch (error) {
    console.error('âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
    console.error('   ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ Ollamaë¥¼ ì‹œì‘í•˜ì„¸ìš”: ollama serve')
    process.exit(1)
  }

  // Provider 1: OllamaRAGProvider (Langchain ê¸°ë°˜)
  console.log('ğŸ“¦ Provider 1: OllamaRAGProvider ì´ˆê¸°í™” ì¤‘...')
  const ragServiceOllama = RAGService.getInstance()
  await ragServiceOllama.initialize({
    providerType: 'ollama',
    vectorStoreId: 'qwen3-embedding-0.6b',
    ollamaEndpoint: 'http://localhost:11434',
    embeddingModel: 'mxbai-embed-large',
    inferenceModel: 'qwen2.5',
    topK: 5,
  })
  console.log('âœ… OllamaRAGProvider ì´ˆê¸°í™” ì™„ë£Œ\n')

  // Provider 2: LangGraphOllamaProvider (LangGraph ê¸°ë°˜)
  console.log('ğŸ“¦ Provider 2: LangGraphOllamaProvider ì´ˆê¸°í™” ì¤‘...')
  await ragServiceOllama.shutdown() // ê¸°ì¡´ Provider ì •ë¦¬
  const ragServiceLangGraph = RAGService.getInstance()
  await ragServiceLangGraph.initialize({
    providerType: 'langgraph',
    vectorStoreId: 'qwen3-embedding-0.6b',
    ollamaEndpoint: 'http://localhost:11434',
    embeddingModel: 'mxbai-embed-large',
    inferenceModel: 'qwen2.5',
    topK: 5,
  })
  console.log('âœ… LangGraphOllamaProvider ì´ˆê¸°í™” ì™„ë£Œ\n')

  console.log('=' .repeat(80))
  console.log('ğŸ”¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ (ê° ì¿¼ë¦¬ë‹¹ 5íšŒ ì¸¡ì •)')
  console.log('=' .repeat(80))
  console.log()

  const allResultsOllama = []
  const allResultsLangGraph = []

  for (const query of TEST_QUERIES) {
    console.log(`\nğŸ“Š ì¿¼ë¦¬: "${query}"`)
    console.log('-' .repeat(80))

    // OllamaRAGProvider í…ŒìŠ¤íŠ¸
    console.log('\n  [Ollama Provider]')
    await ragServiceOllama.shutdown()
    await ragServiceOllama.initialize({
      providerType: 'ollama',
      vectorStoreId: 'qwen3-embedding-0.6b',
      ollamaEndpoint: 'http://localhost:11434',
      embeddingModel: 'mxbai-embed-large',
      inferenceModel: 'qwen2.5',
      topK: 5,
    })

    const ollamaProvider = ragServiceOllama.getOllamaProvider()
    const ollamaResults = []

    for (let i = 0; i < 5; i++) {
      const result = await measurePerformance(ollamaProvider, query, 'ollama')
      ollamaResults.push(result)
      if (result.success) {
        console.log(`    Run ${i + 1}: ${result.elapsed}ms (sources: ${result.sourcesCount})`)
      } else {
        console.log(`    Run ${i + 1}: âŒ ${result.error}`)
      }
    }

    allResultsOllama.push(...ollamaResults.filter((r) => r.success))

    // LangGraphOllamaProvider í…ŒìŠ¤íŠ¸
    console.log('\n  [LangGraph Provider]')
    await ragServiceLangGraph.shutdown()
    await ragServiceLangGraph.initialize({
      providerType: 'langgraph',
      vectorStoreId: 'qwen3-embedding-0.6b',
      ollamaEndpoint: 'http://localhost:11434',
      embeddingModel: 'mxbai-embed-large',
      inferenceModel: 'qwen2.5',
      topK: 5,
    })

    const langgraphProvider = ragServiceLangGraph.getOllamaProvider()
    const langgraphResults = []

    for (let i = 0; i < 5; i++) {
      const result = await measurePerformance(langgraphProvider, query, 'langgraph')
      langgraphResults.push(result)
      if (result.success) {
        console.log(`    Run ${i + 1}: ${result.elapsed}ms (sources: ${result.sourcesCount})`)
      } else {
        console.log(`    Run ${i + 1}: âŒ ${result.error}`)
      }
    }

    allResultsLangGraph.push(...langgraphResults.filter((r) => r.success))

    // ì¿¼ë¦¬ë³„ ìš”ì•½
    const ollamaStats = calculateStats(ollamaResults.filter((r) => r.success))
    const langgraphStats = calculateStats(langgraphResults.filter((r) => r.success))

    console.log('\n  [ì¿¼ë¦¬ë³„ ìš”ì•½]')
    console.log(`    Ollama:    í‰ê·  ${ollamaStats.avg}ms (ìµœì†Œ ${ollamaStats.min}ms, ìµœëŒ€ ${ollamaStats.max}ms)`)
    console.log(`    LangGraph: í‰ê·  ${langgraphStats.avg}ms (ìµœì†Œ ${langgraphStats.min}ms, ìµœëŒ€ ${langgraphStats.max}ms)`)

    const improvement = ((ollamaStats.avg - langgraphStats.avg) / ollamaStats.avg * 100).toFixed(1)
    if (langgraphStats.avg < ollamaStats.avg) {
      console.log(`    âœ… LangGraph ${improvement}% ë¹ ë¦„`)
    } else {
      console.log(`    âš ï¸ Ollama ${Math.abs(improvement)}% ë¹ ë¦„`)
    }
  }

  console.log('\n' + '=' .repeat(80))
  console.log('ğŸ“ˆ ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼')
  console.log('=' .repeat(80))

  const overallOllama = calculateStats(allResultsOllama)
  const overallLangGraph = calculateStats(allResultsLangGraph)

  console.log('\n[OllamaRAGProvider (Langchain ê¸°ë°˜)]')
  console.log(`  í‰ê·  ì‘ë‹µ ì‹œê°„: ${overallOllama.avg}ms`)
  console.log(`  ìµœì†Œ ì‘ë‹µ ì‹œê°„: ${overallOllama.min}ms`)
  console.log(`  ìµœëŒ€ ì‘ë‹µ ì‹œê°„: ${overallOllama.max}ms`)
  console.log(`  í‘œì¤€ í¸ì°¨:     ${overallOllama.stdDev}ms`)
  console.log(`  ì¸¡ì • íšŸìˆ˜:     ${overallOllama.count}íšŒ`)

  console.log('\n[LangGraphOllamaProvider (LangGraph ê¸°ë°˜)]')
  console.log(`  í‰ê·  ì‘ë‹µ ì‹œê°„: ${overallLangGraph.avg}ms`)
  console.log(`  ìµœì†Œ ì‘ë‹µ ì‹œê°„: ${overallLangGraph.min}ms`)
  console.log(`  ìµœëŒ€ ì‘ë‹µ ì‹œê°„: ${overallLangGraph.max}ms`)
  console.log(`  í‘œì¤€ í¸ì°¨:     ${overallLangGraph.stdDev}ms`)
  console.log(`  ì¸¡ì • íšŸìˆ˜:     ${overallLangGraph.count}íšŒ`)

  const totalImprovement =
    ((overallOllama.avg - overallLangGraph.avg) / overallOllama.avg * 100).toFixed(1)

  console.log('\n[ì„±ëŠ¥ ë¹„êµ]')
  if (overallLangGraph.avg < overallOllama.avg) {
    console.log(`  âœ… LangGraphê°€ í‰ê·  ${totalImprovement}% ë¹ ë¦„`)
    console.log(`  ì ˆëŒ€ ì‹œê°„: ${overallOllama.avg - overallLangGraph.avg}ms ë‹¨ì¶•`)
  } else {
    console.log(`  âš ï¸ Ollamaê°€ í‰ê·  ${Math.abs(totalImprovement)}% ë¹ ë¦„`)
    console.log(`  ì ˆëŒ€ ì‹œê°„: ${overallLangGraph.avg - overallOllama.avg}ms ì¦ê°€`)
  }

  console.log('\n[ì˜ˆìƒ vs ì‹¤ì¸¡]')
  console.log(`  ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ: 27.3% (110ms â†’ 80ms)`)
  console.log(`  ì‹¤ì¸¡ ì„±ëŠ¥ í–¥ìƒ: ${totalImprovement}%`)

  if (parseFloat(totalImprovement) >= 20) {
    console.log('  âœ… ëª©í‘œ ë‹¬ì„±! (20% ì´ìƒ í–¥ìƒ)')
  } else if (parseFloat(totalImprovement) >= 10) {
    console.log('  ğŸŸ¡ ë¶€ë¶„ ë‹¬ì„± (10-20% í–¥ìƒ)')
  } else {
    console.log('  âŒ ëª©í‘œ ë¯¸ë‹¬ (10% ë¯¸ë§Œ í–¥ìƒ)')
  }

  console.log('\n' + '=' .repeat(80))
  console.log('âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!')
  console.log('=' .repeat(80))

  // ì •ë¦¬
  await ragServiceOllama.shutdown()
  await ragServiceLangGraph.shutdown()
}

// ì‹¤í–‰
runBenchmark().catch((error) => {
  console.error('ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨:', error)
  process.exit(1)
})
